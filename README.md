# Blog-for-adaboost
This is a material index for writing a blog about adaboost.

http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/  第06章：深入浅出ML之Boosting家族，但可以想象的是，随着spark的进一步发展，该分布式计算平台会变的非常重，功能也会越来越多。离专注、专一和极致的解决某类问题越来越远，对每一类问题给出的解决方案并不会特别好。

http://chuansong.me/n/1146911642859 MXNet专栏 | 李沐：深度学习·炼丹入门 

http://geek.csdn.net/news/detail/201207 揭秘Kaggle神器xgboost

http://blog.csdn.net/heyongluoyao8/article/details/49408131 在分类中如何处理训练集中不平衡问题，这里提到了一种人工数据生成办法，SMOTE

https://www.zhihu.com/topic/19719221/hot adaboost,提到泛化误差的问题，讨论了很多。

虽然鲍捷老师一直在倡导智能金融，并身体力行，但我心里一直嘀咕的是，这事不好办。

前一段时间，有道翻译还蛮靠谱的，但现在也不行了，国内学者或者学生越来越喜欢用英文写作了，从国外搬砖，不得不翻译成中文的，如今和向国内学者或学生学习都得用英文了，
这事就累得慌了。有段时间我再想，有道这么厉害，以后不用学英语了，但现在想明白了，是可以不用学英语了，但得交钱。

周志华老师《机器学习》关于adaboost一节介绍的不多，但在25th adaboost讲了很多。

https://cseweb.ucsd.edu/classes/fa01/cse291/AdaBoost.pdf 2001年Freund and Schaipre 写的简介。

http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf MIT adaboost讲义。

http://mccormickml.com/2013/12/13/adaboost-tutorial/ 有一点内容，不重要。

http://www.multiboost.org/ 多分类boost软件。

xgboost，最早用在300k数据集上，用randomforest直接卡死，用xgboost直接通过，很surprise。

https://github.com/luispedro/milk 一款python和c++加速的adaboost软件。

http://totoharyanto.staff.ipb.ac.id/files/2012/10/Building-Machine-Learning-Systems-with-Python-Richert-Coelho.pdf 和上面是同一个作者。

https://www.zhihu.com/question/23003213 回头要看看python和C+要怎样混合编程。

Nov 13th,2017

https://www.zhihu.com/topic/20035241/hot 关于xgboost的讨论，马超讲了vc维，周在《机器学习》计算学习理论一章里讲到vc、Rademacher复杂度、
稳定性。马超：而特征的多样性也正是为什么工业界很少去使用 svm 的一个重要原因之一？

https://github.com/TracyMcgrady6 马超的github？

https://www.jiqizhixin.com/articles/2017-11-08-3 为什么XGBoost在机器学习竞赛中表现如此卓越？挪威科技大学 Didrik Nielsen 的硕士论文《使用 XGBoost 的树提升：为什么 XGBoost 能赢得「每一场」机器学习竞赛？（Tree Boosting With XGBoost - Why Does XGBoost Win "Every" Machine Learning Competition?）》

http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf 搞了半天，还得弄明白VC维

https://github.com/vividfree/alphabet 罗维的github，很多内容，我想知道怎么写C++？博客也很好，就是乱了一点。

http://txshi-mt.com/2017/08/20/NTUML-7-the-VC-Dimension/ 关于VC维，有点难

http://web.cecs.pdx.edu/~mm/MachineLearningWinter2017/EnsembleLearning.pdf 集成学习示例，比较详细

http://web.cecs.pdx.edu/~mm/MachineLearningSpring2017/EnsembleLearningExerciseSolutions.pdf 上例中计算解释

http://lamda.nju.edu.cn/MainPage.ashx 南大机器学习与数据挖掘研究所网站

https://petolau.github.io/Ensemble-of-trees-for-forecasting-time-series/ 集成学习在电力消费上的预测

http://www.liaad.up.pt/area/jgama/InfSys2017.pdf 集成学习在流数据分析的应用

http://docs.w3cub.com/scikit_learn/auto_examples/ensemble/plot_adaboost_hastie_10_2/ Discrete versus Real AdaBoost

https://www.leiphone.com/news/201707/JSAaQzOebhHHKuTN.html 如何用自动机器学习实现神经网络进化 

https://www.leiphone.com/news/201705/NlTc7oObBqh116Z5.html 南京大学俞扬博士万字演讲全文：强化学习前沿（上）

https://github.com/xiahouzuoxin/notes/tree/master/essays 有一些无聊的东西

http://mnews.onlinedown.net/trends/83553.html 微软研院的时空数据分析，很好的资料

https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/ 这里提到boosting确实可以减少方差和偏差。

http://adataanalyst.com/machine-learning/adaboost-python-3/ 介绍了adaboost的python实现

https://github.com/nathanntg/adaboost adaboost复杂实现

https://github.com/alinagithub/Adaboost/blob/master/AdaBoost.py 看不懂实现

https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/ 很多典故

http://xxuan.me/2017-03-24-adaboost.html 中文adaboost，有python代码

https://www.zhihu.com/question/29138020 如何自学python 这个厉害 还有南洋理工大学的博士 分享 也很厉害

https://github.com/lijin-THU 还不错，介绍python和ml

http://www.jianshu.com/u/212ef8ed6ac2 爬虫或python必备

Nov 14th,2017

https://www.leiphone.com/news/201709/QuBqR1h6I7yglwbH.html 周志华CAIS大会的演讲

https://www.leiphone.com/news/201703/CC6gwUhEAPuG1kvN.html 周志华gcforest

https://www.douban.com/doulist/3440234/ 台大机器学习基石

http://blog.csdn.net/wzmsltw/article/details/51039928 周志华《机器学习》一些python代码

https://pan.baidu.com/s/1qYRMLvY#list/path=%2F 周志华《机器学习》讲义

http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/ 在hadoop上实现mapreduce

https://docs.djangoproject.com/en/1.11/intro/tutorial01/ django练习，完成hello world!

Nov 15th, 2017

http://decisiontrees.net/decision-trees-and-data-mining-software/ 想找一些决策树的源码

http://gabrielelanaro.github.io/blog/2016/03/03/decision-trees.html 简洁、清晰

https://github.com/revantkumar/Decision-Tree 这个ID3蛮好

https://github.com/chrisspen/dtree 决策树，复杂一点

http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf 十大算法的论文

Nov 16th, 2017

http://people.duke.edu/~ccc14/sta-663/EMAlgorithm.html 了解一下如何实现EM，这个网站很好，提供了python的计算统计学

https://zhuanlan.zhihu.com/p/28249050 Jensen’s inequality 不等式的理解

Nov 17th,2017

http://www.ruanyifeng.com/blog/2015/02/make.html 阮一峰的make介绍

http://www.stat.columbia.edu/~gelman/research/unpublished/stan-resubmit-JSS1293.pdf duke大学提到哥大的stan，里面的抽样技术很吸引人

周志华《机器学习》P153 朴素贝叶斯分类器，拉普拉斯修正避免了因训练样本不充分而导致概率估值为零的问题

https://wenku.baidu.com/view/5799a87302768e9951e73890.html 贝叶斯方法介绍 85p

http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf EM算法普及

http://blog.csdn.net/u011300443/article/details/46763743 解释了上文里有些概率是怎么产生的

http://tinyheero.github.io/2016/01/03/gmm-em.html 写的很好

http://www.math.leidenuniv.nl/~avdvaart/ASC/EM.pdf

https://math.usask.ca/~longhai/teaching/stat812-1409/rdemo/EM_examples.pdf

Nov 20th,2017

http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html 讲了一些EM，抄书侠，但是很翔实，不像有些人吹牛

偏差-方差分解说明，泛化能力是由学习算法的能力、数据的充分性以及学习任务本身的难度共同决定的。

<a href="https://www.codecogs.com/eqnedit.php?latex=E\left&space;(&space;f;D&space;\right&space;)=&space;bias^{2}\left&space;(&space;x&space;\right&space;)&plus;var\left&space;(&space;x&space;\right&space;)&plus;\varepsilon&space;^{2}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?E\left&space;(&space;f;D&space;\right&space;)=&space;bias^{2}\left&space;(&space;x&space;\right&space;)&plus;var\left&space;(&space;x&space;\right&space;)&plus;\varepsilon&space;^{2}" title="E\left ( f;D \right )= bias^{2}\left ( x \right )+var\left ( x \right )+\varepsilon ^{2}" /></a>

https://www.codecogs.com/latex/eqneditor.php 在线编辑公式用这个

Nov 21th, 2017

https://www.projectrhea.org/rhea/index.php/MLE_Examples:_Binomial_and_Poisson_Distributions_OldKiwi 介绍MLE，求导有错误

https://stats.stackexchange.com/questions/181035/how-to-derive-the-likelihood-function-for-binomial-distribution-for-parameter-es 介绍MLE

https://tech.meituan.com/deep-understanding-of-ffm-principles-and-practices.html 美团技术团队评价FFM
